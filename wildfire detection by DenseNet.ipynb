{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from torch.utils import data as D\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import random\n",
    "import torchsummary\n",
    "from torchsummary import summary\n",
    "\n",
    "print(torch.__version__)\n",
    "device = torch.device('cuda')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Hyper parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 60\n",
    "random_seed = 10\n",
    "initial_lr = 0.1\n",
    "num_epoch = 250\n",
    "num_images = 12620\n",
    "split = round(num_images/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dataset split & Class speration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "transform_validation = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(\n",
    "root='./Dataset/Dataset_train/', transform=transform_train)\n",
    "validset = torchvision.datasets.ImageFolder(\n",
    "root='./Dataset/Dataset_train/', transform=transform_validation)\n",
    "testset = torchvision.datasets.ImageFolder(\n",
    "root='./Dataset/Dataset_test/', transform=transform_test)\n",
    "\n",
    "num_train = len(trainset)\n",
    "indices = list(range(num_train))\n",
    "np.random.seed(random_seed)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "#split = 2524\n",
    "#train_idx_1 = indices[:1346]\n",
    "#train_idx_2 = indices[2693:]\n",
    "#train_idx = train_idx_1 + train_idx_2\n",
    "\n",
    "train_idx = indices[split+1:]\n",
    "valid_idx = indices[:split]\n",
    "\n",
    "num_test = len(testset)\n",
    "indices_test = list(range(num_test))\n",
    "test_idx = indices[:]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=batch_size, sampler=train_sampler, num_workers=4\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    validset, batch_size=batch_size, sampler=valid_sampler, num_workers=4\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=28, shuffle=False, num_workers=4\n",
    ")\n",
    "\n",
    "classes = ('wildfire','nonfire')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DenseNet architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bn_relu_conv(nn.Module):\n",
    "    def __init__(self, nin, nout, kernel_size, stride, padding, bias=False):\n",
    "        super(bn_relu_conv, self).__init__()\n",
    "        self.batch_norm = nn.BatchNorm2d(nin)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.conv = nn.Conv2d(nin, nout, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.batch_norm(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class bottleneck_layer(nn.Sequential):\n",
    "    def __init__(self, nin, growth_rate, drop_rate=0.2):    \n",
    "        super(bottleneck_layer, self).__init__()\n",
    "      \n",
    "        self.add_module('conv_1x1', bn_relu_conv(nin=nin, nout=growth_rate*4, kernel_size=1, stride=1, padding=0, bias=False))\n",
    "        self.add_module('conv_3x3', bn_relu_conv(nin=growth_rate*4, nout=growth_rate, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "      \n",
    "        self.drop_rate = drop_rate\n",
    "      \n",
    "    def forward(self, x):\n",
    "        bottleneck_output = super(bottleneck_layer, self).forward(x)\n",
    "        if self.drop_rate > 0:\n",
    "            bottleneck_output = F.dropout(bottleneck_output, p=self.drop_rate, training=self.training)\n",
    "          \n",
    "        bottleneck_output = torch.cat((x, bottleneck_output), 1)\n",
    "      \n",
    "        return bottleneck_output\n",
    "\n",
    "class Transition_layer(nn.Sequential):\n",
    "    def __init__(self, nin, theta=0.5):    \n",
    "        super(Transition_layer, self).__init__()\n",
    "      \n",
    "        self.add_module('conv_1x1', bn_relu_conv(nin=nin, nout=int(nin*theta), kernel_size=1, stride=1, padding=0, bias=False))\n",
    "        self.add_module('avg_pool_2x2', nn.AvgPool2d(kernel_size=2, stride=2, padding=0))\n",
    "\n",
    "class DenseBlock(nn.Sequential):\n",
    "    def __init__(self, nin, num_bottleneck_layers, growth_rate, drop_rate=0.2):\n",
    "        super(DenseBlock, self).__init__()\n",
    "                        \n",
    "        for i in range(num_bottleneck_layers):\n",
    "            nin_bottleneck_layer = nin + growth_rate * i\n",
    "            self.add_module('bottleneck_layer_%d' % i, bottleneck_layer(nin=nin_bottleneck_layer, growth_rate=growth_rate, drop_rate=drop_rate))\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growth_rate=12, num_layers=10, theta=0.5, drop_rate=0.2, num_classes=10):\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        assert (num_layers - 4) % 6 == 0\n",
    "\n",
    "        num_bottleneck_layers = (num_layers - 4) // 6\n",
    "\n",
    "        self.dense_init = nn.Conv2d(3, growth_rate*2, kernel_size=7, stride=2, padding=3, bias=True)\n",
    "        self.dense_init_2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.dense_block_1 = DenseBlock(nin=growth_rate*2, num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "\n",
    "        nin_transition_layer_1 = (growth_rate*2) + (growth_rate * num_bottleneck_layers) \n",
    "        self.transition_layer_1 = Transition_layer(nin=nin_transition_layer_1, theta=theta)\n",
    "\n",
    "        self.dense_block_2 = DenseBlock(nin=int(nin_transition_layer_1*theta), num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "\n",
    "        nin_transition_layer_2 = int(nin_transition_layer_1*theta) + (growth_rate * num_bottleneck_layers) \n",
    "        self.transition_layer_2 = Transition_layer(nin=nin_transition_layer_2, theta=theta)\n",
    "\n",
    "        self.dense_block_3 = DenseBlock(nin=int(nin_transition_layer_2*theta), num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "\n",
    "        nin_fc_layer = int(nin_transition_layer_2*theta) + (growth_rate * num_bottleneck_layers) \n",
    "\n",
    "        self.fc_layer = nn.Linear(nin_fc_layer, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        dense_init_output = self.dense_init(x)\n",
    "        dense_init_output_2 = self.dense_init_2(dense_init_output)\n",
    "\n",
    "        dense_block_1_output = self.dense_block_1(dense_init_output_2)\n",
    "        transition_layer_1_output = self.transition_layer_1(dense_block_1_output)\n",
    "\n",
    "        dense_block_2_output = self.dense_block_2(transition_layer_1_output)\n",
    "        transition_layer_2_output = self.transition_layer_2(dense_block_2_output)\n",
    "\n",
    "        dense_block_3_output = self.dense_block_3(transition_layer_2_output)\n",
    "\n",
    "        global_avg_pool_output = F.adaptive_avg_pool2d(dense_block_3_output, (1, 1))                \n",
    "        global_avg_pool_output_flat = global_avg_pool_output.view(global_avg_pool_output.size(0), -1)\n",
    "\n",
    "        output = self.fc_layer(global_avg_pool_output_flat)\n",
    "\n",
    "        return output\n",
    "    \n",
    "def Optimized_DenseNet():\n",
    "    return DenseNet(growth_rate=24, num_layers=100, theta=0.5, drop_rate=0.2, num_classes=2)\n",
    "\n",
    "net = Optimized_DenseNet()\n",
    "densenet = net.to(device)\n",
    "densenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchsummary.summary(net, (224,224,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training & Save the trained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=initial_lr, momentum=0.9)\n",
    "lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[int(num_epoch * 0.5), int(num_epoch * 0.75)], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "for epoch in range(num_epoch):  \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        show_period = 180\n",
    "        if i % show_period == show_period-1:    # print every \"show_period\" mini-batches\n",
    "           \n",
    "            running_loss = 0.0\n",
    "        \n",
    "        \n",
    "    # validation part\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(valid_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "\n",
    "path = \"./savemodel/model.pth\"\n",
    "torch.save(net.state_dict(),path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load model & test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./savemodel/model.pth\"\n",
    "net.load_state_dict(torch.load(path))\n",
    "net.eval()\n",
    "\n",
    "class_correct = list(0. for i in range(2))\n",
    "class_total = list(0. for i in range(2))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        \n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        \n",
    "        _, predicted = torch.max(outputs,1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        print(torch.nn.functional.softmax(outputs, dim=1))\n",
    "        print(predicted)     \n",
    "        for i in range(labels.shape[0]):\n",
    "            \n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "  \n",
    "print('Accuracy of the network on the 1031 test images: %5s %%' % (\n",
    "    100 * correct / total))            \n",
    "            \n",
    "for i in range(2):\n",
    "    print('Accuracy of %5s : %5s %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "    print(class_correct[i])\n",
    "    print(class_total[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Class activation map**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAM(nn.Module):\n",
    "    def __init__(self, model_to_convert, get_fc_layer=lambda m: m.fc_layer,score_fn=F.softmax, resize=True):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(*list(model_to_convert.children())[:-1])\n",
    "        self.fc = get_fc_layer(model_to_convert)\n",
    "        self.conv  =  nn.Conv2d(self.fc.in_features, self.fc.out_features, kernel_size=1)\n",
    "        self.conv.weight = nn.Parameter(self.fc.weight.data.unsqueeze(-1).unsqueeze(-1))\n",
    "        self.conv.bias = self.fc.bias\n",
    "        self.score_fn = score_fn\n",
    "        self.resize = resize\n",
    "        self.eval()\n",
    "        \n",
    "    def forward(self, x, out_size=None):\n",
    "        batch_size, c, *size = x.size()\n",
    "        feat = self.backbone(x)\n",
    "        cmap = self.score_fn(self.conv(feat))\n",
    "        if self.resize:\n",
    "            if out_size is None:\n",
    "                out_size = size\n",
    "            cmap = F.upsample(cmap, size=out_size, mode='bicubic')\n",
    "        pooled = F.adaptive_avg_pool2d(feat,output_size=1)\n",
    "        flatten = pooled.view(batch_size, -1)\n",
    "        cls_score = self.score_fn(self.fc(flatten))\n",
    "        weighted_cmap =  (cmap*cls_score.unsqueeze(-1).unsqueeze(-1)).sum(dim=1)\n",
    "        return cmap, cls_score, weighted_cmap\n",
    "    \n",
    "path = \"./savemodel/model.pth\"\n",
    "net.load_state_dict(torch.load(path))\n",
    "cam = CAM(net)\n",
    "#assert not cam.training\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"use gpu\")\n",
    "    cam = cam.cuda()\n",
    "    def to_var(x, requires_grad=False, volatile=False):\n",
    "        return Variable(x.cuda(), requires_grad=requires_grad, volatile=volatile)\n",
    "else:\n",
    "    def to_var(x, requires_grad=False, volatile=False):\n",
    "        return Variable(x, requires_grad=requirs_grad, volatile=volatile)\n",
    "\n",
    "\n",
    "target_size = (224,224)\n",
    "\n",
    "normalize = transforms.Normalize([0.5, 0.5, 0.5],\n",
    "                                 [0.5, 0.5, 0.5])\n",
    "transform = transforms.Compose([transforms.Scale(target_size),transforms.CenterCrop(target_size),\n",
    "                                transforms.ToTensor()])\n",
    "from torch.autograd import Variable\n",
    "\n",
    "img_path = \"./Dataset/Dataset_test/wildfire/00014.jpg\"\n",
    "img = Image.open(img_path)\n",
    "img_v = to_var(transform(img).unsqueeze(0),volatile=True)\n",
    "\n",
    "cmap, score, weighted_cmap = cam(img_v)\n",
    "print(cmap.size())\n",
    "print(score.size())\n",
    "print(weighted_cmap.size())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "background = np.array(img.resize(target_size))\n",
    "color_map = weighted_cmap.data.cpu().numpy()[0]\n",
    "color_map = cmap.data.cpu().numpy()[0,1]\n",
    "#print(color_map)\n",
    "ax = plt.gca()\n",
    "ax.axes.xaxis.set_visible(False)\n",
    "ax.axes.yaxis.set_visible(False)\n",
    "plt.imshow(background)\n",
    "plt.imshow(color_map,cmap ='jet',alpha=0.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
